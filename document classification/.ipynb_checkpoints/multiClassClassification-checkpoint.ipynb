{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#import featureExtractor.tfidf as tfidf\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def readData(dataPath):\n",
    "    #input: file path \n",
    "    #output: returns a spark dataframe \n",
    "    data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(dataPath)\n",
    "    print(\"Data Read successfully ...\")\n",
    "    drop_list = ['url']\n",
    "    \n",
    "    data = data.select([column for column in data.columns if column not in drop_list])\n",
    "    return data.dropna()\n",
    "\n",
    "\n",
    "\n",
    "def tfidf(regexTokenizer, stopwordsRemover, label_stringIdx):\n",
    "    #returns pipeline using tfidf \n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "    return Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "def countVectorizer(regexTokenizer, stopwordsRemover, label_stringIdx):\n",
    "    # bag of words count\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "    label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "    return Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])   \n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    special = ['ms','mr','http','https','amp', 'none', 'i’m', 'th','don’t','it’s','advertisement']\n",
    "    with open('seostopwords.txt') as f:\n",
    "        stopLines = f.read().splitlines()\n",
    "\n",
    "    dump = [stop_words.add(i) for i in special]\n",
    "    dump = [stop_words.add(i) for i in stopLines]\n",
    "    return list(stop_words)\n",
    "    \n",
    "def classify(featureExtractor, classifier):\n",
    "    #input : \n",
    "        #1)feature extractor (count vectorizer or tfidf)\n",
    "        #2) classifier algorithm (random forest, naive bayes or linear reg)\n",
    "        \n",
    "    # training :80% , testing 10% , validation 10%\n",
    "    \n",
    "    \n",
    "    #Part 1:  Combine data from all sources \n",
    "    data_politics = readData('politics.csv')\n",
    "    data_sports = readData('sports.csv')\n",
    "    data_business = readData('business.csv')\n",
    "    data_movies = readData('movies.csv')\n",
    "    data_travel_tourism = readData('tourism.csv')\n",
    "    \n",
    "    \n",
    "    data_merged = data_politics.union(data_sports)\n",
    "    data_merged = data_merged.union(data_business)\n",
    "    data_merged = data_merged.union(data_movies)\n",
    "    data_merged = data_merged.union(data_travel_tourism)\n",
    "    \n",
    "    data = data_merged.withColumnRenamed(\"category\", \"Category\").withColumnRenamed(\"content\", \"Descript\")\n",
    "    print(\"The total number of articles from 4 categories is \" , data.count())\n",
    "        \n",
    "    # Part 2 -> build regex tokenizer \n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    \n",
    "    # stop words ---------- (3)\n",
    "    add_stopwords = get_stopwords()\n",
    "    #print(add_stopwords)\n",
    "    \n",
    "    # Part 3 -> build stop word tokenizer \n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords=add_stopwords)\n",
    "    #print(stopwordsRemover.transform())\n",
    "    label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "\n",
    "    # Part 4 -> build pipeline \n",
    "    if featureExtractor==\"cv\":\n",
    "        pipeline = countVectorizer(regexTokenizer, stopwordsRemover, label_stringIdx)\n",
    "    elif featureExtractor==\"tfidf\":\n",
    "        pipeline = tfidf(regexTokenizer, stopwordsRemover, label_stringIdx)\n",
    "    print(\"Pipeline constructed successfully..\")\n",
    "\n",
    "    # Fit the pipeline to the model.\n",
    "    pipelineFit = pipeline.fit(data)\n",
    "    \n",
    "    dataset = pipelineFit.transform(data)\n",
    "    # Part 5 -> split data and classify\n",
    "    (trainingData, validationData, testData) = dataset.randomSplit([0.8, 0.1, 0.1], seed = 100)\n",
    "    \n",
    "    \n",
    "    print(\"Training model..\")\n",
    "    if classifier==\"lr\":\n",
    "        selectedModel = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "    elif classifier==\"nb\":\n",
    "        selectedModel = NaiveBayes(smoothing=1)\n",
    "    elif classifier==\"rf\":\n",
    "        selectedModel = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 20, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "    model = selectedModel.fit(trainingData)\n",
    "    print(\"Model Trained..\")\n",
    "    \n",
    "    # part 6 -> predict and test model\n",
    "    print(\"Test model..\")\n",
    "    predictions = model.transform(validationData)\n",
    "    predictions.filter(predictions['prediction'] == 0) \\\n",
    "        .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "        .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    print(\"Evaluate model..\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy: \"+str(accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Please select method for feature extraction. Type\\ncv for count vectorizer\\ntfidf for term \\\n",
    "frequency inverse document frequency\")\n",
    "    fe_choice = input()\n",
    "    print(\"\\nPlese enter algorithm for classification. Type\\nlr for linear regression\\nnb for naive bayes\\n\\\n",
    "rf for random forest\")\n",
    "    algo_choice = input()\n",
    "    classify(fe_choice,algo_choice)\n",
    "    sc.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select method for feature extraction. Type\n",
      "cv for count vectorizer\n",
      "tfidf for term frequency inverse document frequency\n",
      "tfidf\n",
      "\n",
      "Plese enter algorithm for classification. Type\n",
      "lr for linear regression\n",
      "nb for naive bayes\n",
      "rf for random forest\n",
      "nb\n",
      "Data Read successfully ...\n",
      "Data Read successfully ...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/media/g1994/Windows/Gautam/Masters/spring 18/cse 587/Lab3/Lab3/Part2/business.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/media/g1994/Windows/Gautam/Masters/spring 18/cse 587/Lab3/Lab3/Part2/business.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5a0942504740>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPlese enter algorithm for classification. Type\\nlr for linear regression\\nnb for naive bayes\\nrf for random forest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malgo_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfe_choice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0b267534ea3e>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(featureExtractor, classifier)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdata_politics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'politics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mdata_sports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sports.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdata_business\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'business.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mdata_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mdata_travel_tourism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tourism.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0b267534ea3e>\u001b[0m in \u001b[0;36mreadData\u001b[0;34m(dataPath)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#input: file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#output: returns a spark dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Read successfully ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdrop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/media/g1994/Windows/Gautam/Masters/spring 18/cse 587/Lab3/Lab3/Part2/business.csv;'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cv    tfidf\n",
    "lr 0.631  0.733\n",
    "nb 0.654  0.692\n",
    "rf 0.737  0.732"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
