{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import os.path\n",
    "\n",
    "def scrapeHTML(url,category):\n",
    "    text = \"\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page,\"html\")\n",
    "    heading = soup.h1.string\n",
    "    contents = soup.find_all(\"p\")\n",
    "    for content in contents:\n",
    "        text = text+str(content.string)+\" \"\n",
    "    heading = re.sub(',' , ' ', heading)\n",
    "    heading = re.sub(';' , ' ', heading)\n",
    "    text = re.sub(',' , ' ', text)\n",
    "    text = re.sub(';' , ' ', text)\n",
    "    Page = {'url':url, 'content':heading+ \" \" + text, 'category':category}\n",
    "    return Page\n",
    "\n",
    "\n",
    "\n",
    "def scraping(query,filename):\n",
    "\n",
    "    Pages = pd.DataFrame(columns = ['url','content'])\n",
    "\n",
    "\n",
    "    api_key =\"3a2aeebff29144ab99ef0fe96f89ff24\"\n",
    "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=\"+api_key+\"&q=\"+query\n",
    "    response = requests.get(url)\n",
    "    response_data = response.content\n",
    "    json_data =response_data.decode('utf8') # converting data to string\n",
    "    data = json.loads(json_data);\n",
    "    s = json.dumps(data, indent = 2, )\n",
    "    data_dump = data['response']['docs']\n",
    "    for i in data_dump:\n",
    "        print(i['web_url'])\n",
    "        try:\n",
    "            Pages = Pages.append(scrapeHTML(i['web_url'],filename.replace('.csv','')), ignore_index = True)\n",
    "        except:\n",
    "            continue;\n",
    "    \n",
    "    my_file = os.path.isfile(filename)\n",
    "    if(my_file):\n",
    "        old = pd.read_csv(filename)\n",
    "        upd = old.append(Pages)\n",
    "    else:\n",
    "        upd = pd.DataFrame(Pages)\n",
    "    \n",
    "    upd = upd.drop_duplicates(subset = ['url'],keep = 'first')\n",
    "    upd.to_csv(filename,index = False)\n",
    "    \n",
    "#old = pd.read_csv('../data/data.csv')\n",
    "#print(old)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sports_list = ['sports','football','cricket','hockey','soccer','rugby','golf','tennis']\n",
    "politics_list = ['politics','elections','trump','election','obama','democrat','congress','assembly']\n",
    "business_list = ['business','nasdaq','stocks','enterprise','market share','sales','finance','annual growth']\n",
    "movies_list = ['movie', 'avengers', 'drama','romcom','suspense','thriller','bollywood','batman','hollywood']\n",
    "tourism_list = ['tourism','holiday spots','vacation spots']\n",
    "technology_list = ['cell phone','android','apple','google','internet', 'technology','virtual reality','electronics']\n",
    "students_and_education_list = ['education', 'university students', 'learning high school', 'college grades', 'classroom teaching','university degree','exams']\n",
    "med_and_health_list = ['health','medicine','medicinal drugs','surgery','medical illness']\n",
    "assault_list = ['rape','killed','violence','murdered','assault']\n",
    "terrorism_list = ['terrorist','isis','al qaida','bombings', 'boko haram', 'osama bin laden','terrorism']\n",
    "\n",
    "#for i in politics_list:\n",
    "#    scraping(i, 'politics.csv')\n",
    "    \n",
    "#for j in sports_list:\n",
    "#    scraping(j, 'sports.csv')\n",
    "    \n",
    "#for k in business_list:\n",
    "#    scraping(k,'business.csv')\n",
    "    \n",
    "#for i in movies_list:\n",
    "#    scraping(i,'movies.csv')\n",
    "    \n",
    "#for i in tourism_list:\n",
    "#    scraping(i,'tourism.csv')\n",
    "\n",
    "#for i in technology_list:\n",
    "#    scraping(i, 'technology.csv')\n",
    "\n",
    "#for i in students_and_education_list:\n",
    "#    scraping(i, 'students and education.csv')\n",
    "\n",
    "#for i in med_and_health_list:\n",
    "#    scraping(i, 'med_and_health.csv')\n",
    "\n",
    "#for i in assault_list:\n",
    "#    scraping(i, 'assault.csv')\n",
    "    \n",
    "for i in terrorism_list:\n",
    "    scraping(i, 'terrorism.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
